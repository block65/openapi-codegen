/**
 * This file was auto generated by @block65/openapi-codegen
 *
 * WARN: Do not edit directly.
 *
 * Generated on 2024-07-20T03:14:00.890Z
 *
 */
import type { Jsonifiable, Jsonify } from 'type-fest';
import type { JsonifiableObject } from 'type-fest/source/jsonifiable.js';

export type DeleteModelResponse = {
  id: string;
  deleted: boolean;
  object: string;
};
export type CreateCompletionRequest = {
  model:
    | string
    | 'babbage-002'
    | 'davinci-002'
    | 'gpt-3.5-turbo-instruct'
    | 'text-davinci-003'
    | 'text-davinci-002'
    | 'text-davinci-001'
    | 'code-davinci-002'
    | 'text-curie-001'
    | 'text-babbage-001'
    | 'text-ada-001'
    | null;
  /**
   * The prompt(s) to generate completions for, encoded as a string, array of
   * strings, array of tokens, or array of token arrays.
   *
   * Note that <|endoftext|> is the document separator that the model sees
   * during training, so if a prompt is not specified the model will generate as
   * if from the beginning of a new document.
   * @default <|endoftext|>
   */
  prompt: string | string[] | number[] | number[][] | null;
  /**
   * Generates `best_of` completions server-side and returns the "best" (the one
   * with the highest log probability per token). Results cannot be streamed.
   *
   * When used with `n`, `best_of` controls the number of candidate completions
   * and `n` specifies how many to return â€“ `best_of` must be greater than `n`.
   *
   * **Note:** Because this parameter generates many completions, it can quickly
   * consume your token quota. Use carefully and ensure that you have reasonable
   * settings for `max_tokens` and `stop`.
   * @default 1
   */
  best_of?: number | null;
  echo?: boolean | null | undefined;
  frequency_penalty?: number | null;
  logit_bias?: JsonifiableObject | null;
  logprobs?: number | null;
  /**
   * The maximum number of [tokens](/tokenizer) to generate in the completion.
   *
   * The token count of your prompt plus `max_tokens` cannot exceed the model's
   * context length. [Example Python
   * code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
   * for counting tokens.
   * @default 16
   * @example 16
   */
  max_tokens?: number | null;
  /**
   * How many completions to generate for each prompt.
   *
   * **Note:** Because this parameter generates many completions, it can quickly
   * consume your token quota. Use carefully and ensure that you have reasonable
   * settings for `max_tokens` and `stop`.
   * @default 1
   * @example 1
   */
  n?: number | null;
  presence_penalty?: number | null;
  seed?: number | null;
  stop?: string | null | string[] | null | undefined;
  stream?: boolean | null | undefined;
  /**
   * The suffix that comes after a completion of inserted text.
   * @example test.
   */
  suffix?: string | null | undefined;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8
   * will make the output more random, while lower values like 0.2 will make it
   * more focused and deterministic.
   *
   * We generally recommend altering this or `top_p` but not both.
   * @default 1
   * @example 1
   */
  temperature?: number | null;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where
   * the model considers the results of the tokens with top_p probability mass.
   * So 0.1 means only the tokens comprising the top 10% probability mass are
   * considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   * @default 1
   * @example 1
   */
  top_p?: number | null;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to
   * monitor and detect abuse. [Learn
   * more](/docs/guides/safety-best-practices/end-user-ids).
   * @example user-1234
   */
  user?: string | undefined;
};

/** The role of the author of a message */
export enum ChatCompletionRole {
  System = 'system',
  User = 'user',
  Assistant = 'assistant',
  Tool = 'tool',
  Function = 'function',
}

/**
 * Represents a streamed chunk of a chat completion response returned by
 * model, based on the provided input.
 */
export type CreateChatCompletionStreamResponse = {
  id: string;
  choices: Array<{
    /** WARN: $ref used before available - #/components/schemas/ChatCompletionStreamResponseDelta */
    delta: never;
    /**
     * The reason the model stopped generating tokens. This will be `stop` if the
     * model hit a natural stop point or a provided stop sequence,
     * `length` if the maximum number of tokens specified in the request was
     * reached,
     * `content_filter` if content was omitted due to a flag from our content
     * filters,
     * `tool_calls` if the model called a tool, or `function_call` (deprecated) if
     * the model called a function.
     * @enum stop,length,tool_calls,content_filter,function_call
     */
    finish_reason:
      | 'stop'
      | 'length'
      | 'tool_calls'
      | 'content_filter'
      | 'function_call';
    index: number;
  }>;
  created: number;
  model: string;
  system_fingerprint?: string | undefined;
  /**
   * The object type, which is always `chat.completion.chunk`.
   * @enum chat.completion.chunk
   */
  object: 'chat.completion.chunk';
};
/**
 * Represents a streamed chunk of a chat completion response returned by
 * model, based on the provided input.
 */
export type CreateChatCompletionImageResponse = {};
export type CreateEditRequest = {
  /**
   * The instruction that tells the model how to edit the prompt.
   * @example Fix the spelling mistakes.
   */
  instruction: string;
  /**
   * ID of the model to use. You can use the `text-davinci-edit-001` or
   * `code-davinci-edit-001` model with this endpoint.
   * @example text-davinci-edit-001
   */
  model: string | 'text-davinci-edit-001' | 'code-davinci-edit-001' | null;
  /**
   * The input text to use as a starting point for the edit.
   * @example What day of the wek is it?
   */
  input?: string | null | undefined;
  /**
   * How many edits to generate for the input and instruction.
   * @default 1
   * @example 1
   */
  n?: number | null;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8
   * will make the output more random, while lower values like 0.2 will make it
   * more focused and deterministic.
   *
   * We generally recommend altering this or `top_p` but not both.
   * @default 1
   * @example 1
   */
  temperature?: number | null;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where
   * the model considers the results of the tokens with top_p probability mass.
   * So 0.1 means only the tokens comprising the top 10% probability mass are
   * considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   * @default 1
   * @example 1
   */
  top_p?: number | null;
};
export type CreateImageRequest = {
  /**
   * A text description of the desired image(s). The maximum length is 1000
   * characters for `dall-e-2` and 4000 characters for `dall-e-3`.
   * @example A cute baby sea otter
   */
  prompt: string;
  /**
   * The model to use for image generation.
   * @default dall-e-2
   * @example dall-e-3
   */
  model?: string | 'dall-e-2' | 'dall-e-3' | null | undefined;
  /**
   * The number of images to generate. Must be between 1 and 10. For `dall-e-3`,
   * only `n=1` is supported.
   * @default 1
   * @example 1
   */
  n?: number | null;
  /**
   * The quality of the image that will be generated. `hd` creates images with
   * finer details and greater consistency across the image. This param is only
   * supported for `dall-e-3`.
   * @default standard
   * @enum standard,hd
   * @example standard
   */
  quality?: 'standard' | 'hd' | undefined;
  /**
   * The format in which the generated images are returned. Must be one of `url`
   * or `b64_json`.
   * @default url
   * @enum url,b64_json
   * @example url
   */
  response_format?: 'url' | 'b64_json' | undefined;
  /**
   * The size of the generated images. Must be one of `256x256`, `512x512`, or
   * `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or
   * `1024x1792` for `dall-e-3` models.
   * @default 1024x1024
   * @enum 256x256,512x512,1024x1024,1792x1024,1024x1792
   * @example 1024x1024
   */
  size?:
    | '256x256'
    | '512x512'
    | '1024x1024'
    | '1792x1024'
    | '1024x1792'
    | undefined;
  /**
   * The style of the generated images. Must be one of `vivid` or `natural`.
   * Vivid causes the model to lean towards generating hyper-real and dramatic
   * images. Natural causes the model to produce more natural, less hyper-real
   * looking images. This param is only supported for `dall-e-3`.
   * @default vivid
   * @enum vivid,natural
   * @example vivid
   */
  style?: 'vivid' | 'natural' | undefined;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to
   * monitor and detect abuse. [Learn
   * more](/docs/guides/safety-best-practices/end-user-ids).
   * @example user-1234
   */
  user?: string | undefined;
};
export type CreateImageEditRequest = {
  image: string;
  /**
   * A text description of the desired image(s). The maximum length is 1000
   * characters.
   * @example A cute baby sea otter wearing a beret
   */
  prompt: string;
  mask?: string | undefined;
  /**
   * The model to use for image generation. Only `dall-e-2` is supported at this
   * time.
   * @default dall-e-2
   * @example dall-e-2
   */
  model?: string | 'dall-e-2' | null | undefined;
  /**
   * The number of images to generate. Must be between 1 and 10.
   * @default 1
   * @example 1
   */
  n?: number | null;
  /**
   * The size of the generated images. Must be one of `256x256`, `512x512`, or
   * `1024x1024`.
   * @default 1024x1024
   * @enum 256x256,512x512,1024x1024
   * @example 1024x1024
   */
  size?: '256x256' | '512x512' | '1024x1024' | undefined;
  /**
   * The format in which the generated images are returned. Must be one of `url`
   * or `b64_json`.
   * @default url
   * @enum url,b64_json
   * @example url
   */
  response_format?: 'url' | 'b64_json' | undefined;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to
   * monitor and detect abuse. [Learn
   * more](/docs/guides/safety-best-practices/end-user-ids).
   * @example user-1234
   */
  user?: string | undefined;
};
export type CreateImageVariationRequest = {
  image: string;
  /**
   * The model to use for image generation. Only `dall-e-2` is supported at this
   * time.
   * @default dall-e-2
   * @example dall-e-2
   */
  model?: string | 'dall-e-2' | null | undefined;
  /**
   * The number of images to generate. Must be between 1 and 10. For `dall-e-3`,
   * only `n=1` is supported.
   * @default 1
   * @example 1
   */
  n?: number | null;
  /**
   * The format in which the generated images are returned. Must be one of `url`
   * or `b64_json`.
   * @default url
   * @enum url,b64_json
   * @example url
   */
  response_format?: 'url' | 'b64_json' | undefined;
  /**
   * The size of the generated images. Must be one of `256x256`, `512x512`, or
   * `1024x1024`.
   * @default 1024x1024
   * @enum 256x256,512x512,1024x1024
   * @example 1024x1024
   */
  size?: '256x256' | '512x512' | '1024x1024' | undefined;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to
   * monitor and detect abuse. [Learn
   * more](/docs/guides/safety-best-practices/end-user-ids).
   * @example user-1234
   */
  user?: string | undefined;
};
export type CreateModerationRequest = {
  input: string | string[] | null;
  /**
   * Two content moderations models are available: `text-moderation-stable` and
   * `text-moderation-latest`.
   *
   * The default is `text-moderation-latest` which will be automatically
   * upgraded over time. This ensures you are always using our most accurate
   * model. If you use `text-moderation-stable`, we will provide advanced notice
   * before updating the model. Accuracy of `text-moderation-stable` may be
   * slightly lower than for `text-moderation-latest`.
   * @default text-moderation-latest
   * @example text-moderation-stable
   */
  model?:
    | string
    | 'text-moderation-latest'
    | 'text-moderation-stable'
    | null
    | undefined;
};
/**
 * Represents policy compliance report by OpenAI's content moderation model
 * against a given input.
 */
export type CreateModerationResponse = {
  id: string;
  model: string;
  results: Array<{
    flagged: boolean;
    categories: {
      hate: boolean;
      'hate/threatening': boolean;
      harassment: boolean;
      'harassment/threatening': boolean;
      'self-harm': boolean;
      'self-harm/intent': boolean;
      'self-harm/instructions': boolean;
      sexual: boolean;
      'sexual/minors': boolean;
      violence: boolean;
      'violence/graphic': boolean;
    };
    category_scores: {
      hate: number;
      'hate/threatening': number;
      harassment: number;
      'harassment/threatening': number;
      'self-harm': number;
      'self-harm/intent': number;
      'self-harm/instructions': number;
      sexual: number;
      'sexual/minors': number;
      violence: number;
      'violence/graphic': number;
    };
  }>;
};
export type CreateFileRequest = {
  file: string;
  /**
   * The intended purpose of the uploaded file.
   *
   * Use "fine-tune" for [Fine-tuning](/docs/api-reference/fine-tuning) and
   * "assistants" for [Assistants](/docs/api-reference/assistants) and
   * [Messages](/docs/api-reference/messages). This allows us to validate the
   * format of the uploaded file is correct for fine-tuning.
   * @enum fine-tune,assistants
   */
  purpose: 'fine-tune' | 'assistants';
};
export type DeleteFileResponse = {
  id: string;
  object: 'file';
  deleted: boolean;
};
export type CreateFineTuningJobRequest = {
  /**
   * The name of the model to fine-tune. You can select one of the
   * [supported models](/docs/guides/fine-tuning/what-models-can-be-fine-tuned).
   * @example gpt-3.5-turbo
   */
  model: string | 'babbage-002' | 'davinci-002' | 'gpt-3.5-turbo' | null;
  /**
   * The ID of an uploaded file that contains training data.
   *
   * See [upload file](/docs/api-reference/files/upload) for how to upload a
   * file.
   *
   * Your dataset must be formatted as a JSONL file. Additionally, you must
   * upload your file with the purpose `fine-tune`.
   *
   * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
   * @example file-abc123
   */
  training_file: string;
  hyperparameters?:
    | {
        /**
         * Number of examples in each batch. A larger batch size means that model
         * parameters
         * are updated less frequently, but with lower variance.
         * @default auto
         */
        batch_size?: 'auto' | number | null | undefined;
        /**
         * Scaling factor for the learning rate. A smaller learning rate may be useful
         * to avoid
         * overfitting.
         * @default auto
         */
        learning_rate_multiplier?: 'auto' | number | null | undefined;
        /**
         * The number of epochs to train the model for. An epoch refers to one full
         * cycle
         * through the training dataset.
         * @default auto
         */
        n_epochs?: 'auto' | number | null | undefined;
      }
    | undefined;
  suffix?: string | null | undefined;
  /**
   * The ID of an uploaded file that contains validation data.
   *
   * If you provide this file, the data is used to generate validation
   * metrics periodically during fine-tuning. These metrics can be viewed in
   * the fine-tuning results file.
   * The same data should not be present in both train and validation files.
   *
   * Your dataset must be formatted as a JSONL file. You must upload your file
   * with the purpose `fine-tune`.
   *
   * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
   * @example file-abc123
   */
  validation_file?: string | null | undefined;
};
export type CreateFineTuneRequest = {
  /**
   * The ID of an uploaded file that contains training data.
   *
   * See [upload file](/docs/api-reference/files/upload) for how to upload a
   * file.
   *
   * Your dataset must be formatted as a JSONL file, where each training
   * example is a JSON object with the keys "prompt" and "completion".
   * Additionally, you must upload your file with the purpose `fine-tune`.
   *
   * See the [fine-tuning
   * guide](/docs/guides/legacy-fine-tuning/creating-training-data) for more
   * details.
   * @example file-abc123
   */
  training_file: string;
  batch_size?: number | null;
  /**
   * If this is provided, we calculate F-beta scores at the specified
   * beta values. The F-beta score is a generalization of F-1 score.
   * This is only used for binary classification.
   *
   * With a beta of 1 (i.e. the F-1 score), precision and recall are
   * given the same weight. A larger beta score puts more weight on
   * recall and less on precision. A smaller beta score puts more weight
   * on precision and less on recall.
   * @example 0.6,1,1.5,2
   */
  classification_betas?: number[];
  classification_n_classes?: number | null;
  classification_positive_class?: string | null | undefined;
  compute_classification_metrics?: boolean | null | undefined;
  hyperparameters?:
    | {
        /**
         * The number of epochs to train the model for. An epoch refers to one
         * full cycle through the training dataset.
         * @default auto
         */
        n_epochs?: 'auto' | number | null | undefined;
      }
    | undefined;
  learning_rate_multiplier?: number | null;
  /**
   * The name of the base model to fine-tune. You can select one of "ada",
   * "babbage", "curie", "davinci", or a fine-tuned model created after
   * 2022-04-21 and before 2023-08-22.
   * To learn more about these models, see the
   * [Models](/docs/models) documentation.
   * @default curie
   * @example curie
   */
  model?: string | 'ada' | 'babbage' | 'curie' | 'davinci' | null | undefined;
  /**
   * The weight to use for loss on the prompt tokens. This controls how
   * much the model tries to learn to generate the prompt (as compared
   * to the completion which always has a weight of 1.0), and can add
   * a stabilizing effect to training when completions are short.
   *
   * If prompts are extremely long (relative to completions), it may make
   * sense to reduce this weight so as to avoid over-prioritizing
   * learning the prompt.
   * @default 0.01
   */
  prompt_loss_weight?: number | null;
  suffix?: string | null | undefined;
  /**
   * The ID of an uploaded file that contains validation data.
   *
   * If you provide this file, the data is used to generate validation
   * metrics periodically during fine-tuning. These metrics can be viewed in
   * the [fine-tuning results
   * file](/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).
   * Your train and validation data should be mutually exclusive.
   *
   * Your dataset must be formatted as a JSONL file, where each validation
   * example is a JSON object with the keys "prompt" and "completion".
   * Additionally, you must upload your file with the purpose `fine-tune`.
   *
   * See the [fine-tuning
   * guide](/docs/guides/legacy-fine-tuning/creating-training-data) for more
   * details.
   * @example file-abc123
   */
  validation_file?: string | null | undefined;
};
export type CreateEmbeddingRequest = {
  /**
   * Input text to embed, encoded as a string or array of tokens. To embed
   * multiple inputs in a single request, pass an array of strings or array of
   * token arrays. The input must not exceed the max input tokens for the model
   * (8192 tokens for `text-embedding-ada-002`), cannot be an empty string, and
   * any array must be 2048 dimensions or less. [Example Python
   * code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
   * for counting tokens.
   * @example The quick brown fox jumped over the lazy dog
   */
  input: string | string[] | number[] | number[][] | null;
  /**
   * ID of the model to use. You can use the [List
   * models](/docs/api-reference/models/list) API to see all of your available
   * models, or see our [Model overview](/docs/models/overview) for descriptions
   * of them.
   * @example text-embedding-ada-002
   */
  model: string | 'text-embedding-ada-002' | null;
  /**
   * The format to return the embeddings in. Can be either `float` or
   * [`base64`](https://pypi.org/project/pybase64/).
   * @default float
   * @enum float,base64
   * @example float
   */
  encoding_format?: 'float' | 'base64' | undefined;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to
   * monitor and detect abuse. [Learn
   * more](/docs/guides/safety-best-practices/end-user-ids).
   * @example user-1234
   */
  user?: string | undefined;
};
export type CreateTranscriptionRequest = {
  file: string;
  /**
   * ID of the model to use. Only `whisper-1` is currently available.
   * @example whisper-1
   */
  model: string | 'whisper-1' | null;
  language?: string | undefined;
  prompt?: string | undefined;
  /**
   * The format of the transcript output, in one of these options: `json`,
   * `text`, `srt`, `verbose_json`, or `vtt`.
   * @default json
   * @enum json,text,srt,verbose_json,vtt
   */
  response_format?:
    | 'json'
    | 'text'
    | 'srt'
    | 'verbose_json'
    | 'vtt'
    | undefined;
  temperature?: number;
};
export type CreateTranscriptionResponse = {
  text: string;
};
export type CreateTranslationRequest = {
  file: string;
  /**
   * ID of the model to use. Only `whisper-1` is currently available.
   * @example whisper-1
   */
  model: string | 'whisper-1' | null;
  prompt?: string | undefined;
  /**
   * The format of the transcript output, in one of these options: `json`,
   * `text`, `srt`, `verbose_json`, or `vtt`.
   * @default json
   */
  response_format?: string | undefined;
  temperature?: number;
};
export type CreateTranslationResponse = {
  text: string;
};
export type CreateSpeechRequest = {
  model: string | 'tts-1' | 'tts-1-hd' | null;
  input: string;
  /**
   * The voice to use when generating the audio. Supported voices are `alloy`,
   * `echo`, `fable`, `onyx`, `nova`, and `shimmer`. Previews of the voices are
   * available in the [Text to speech
   * guide](/docs/guides/text-to-speech/voice-options).
   * @enum alloy,echo,fable,onyx,nova,shimmer
   */
  voice: 'alloy' | 'echo' | 'fable' | 'onyx' | 'nova' | 'shimmer';
  /**
   * The format to audio in. Supported formats are `mp3`, `opus`, `aac`, and
   * `flac`.
   * @default mp3
   * @enum mp3,opus,aac,flac
   */
  response_format?: 'mp3' | 'opus' | 'aac' | 'flac' | undefined;
  /**
   * The speed of the generated audio. Select a value from `0.25` to `4.0`.
   * `1.0` is the default.
   * @default 1
   */
  speed?: number;
};
export type CreateAssistantRequest = {
  model: string;
  name?: string | null | undefined;
  description?: string | null | undefined;
  instructions?: string | null | undefined;
  /**
   * A list of tool enabled on the assistant. There can be a maximum of 128
   * tools per assistant. Tools can be of types `code_interpreter`, `retrieval`,
   * or `function`.
   * @default
   */
  tools?: Array<never | never | never | null>;
  /**
   * A list of [file](/docs/api-reference/files) IDs attached to this assistant.
   * There can be a maximum of 20 files attached to the assistant. Files are
   * ordered by their creation date in ascending order.
   * @default
   */
  file_ids?: string[];
  metadata?: JsonifiableObject | null;
};
export type ModifyAssistantRequest = {
  model?: string | undefined;
  name?: string | null | undefined;
  description?: string | null | undefined;
  instructions?: string | null | undefined;
  /**
   * A list of tool enabled on the assistant. There can be a maximum of 128
   * tools per assistant. Tools can be of types `code_interpreter`, `retrieval`,
   * or `function`.
   * @default
   */
  tools?: Array<never | never | never | null>;
  /**
   * A list of [File](/docs/api-reference/files) IDs attached to this assistant.
   * There can be a maximum of 20 files attached to the assistant. Files are
   * ordered by their creation date in ascending order. If a file was previosuly
   * attached to the list but does not show up in the list, it will be deleted
   * from the assistant.
   * @default
   */
  file_ids?: string[];
  metadata?: JsonifiableObject | null;
};
export type DeleteAssistantResponse = {
  id: string;
  deleted: boolean;
  object: 'assistant.deleted';
};
export type AssistantToolsCode = {
  /**
   * The type of tool being defined: `code_interpreter`
   * @enum code_interpreter
   */
  type: 'code_interpreter';
};
export type AssistantToolsRetrieval = {
  /**
   * The type of tool being defined: `retrieval`
   * @enum retrieval
   */
  type: 'retrieval';
};
export type CreateRunRequest = {
  assistant_id: string;
  model?: string | null | undefined;
  instructions?: string | null | undefined;
  tools?: Array<AssistantToolsCode | AssistantToolsRetrieval | never | null>;
  metadata?: JsonifiableObject | null;
};
export type ModifyRunRequest = {
  metadata?: JsonifiableObject | null;
};
export type SubmitToolOutputsRunRequest = {
  tool_outputs: Array<{
    tool_call_id?: string | undefined;
    output?: string | undefined;
  }>;
};
export type ModifyThreadRequest = {
  metadata?: JsonifiableObject | null;
};
export type DeleteThreadResponse = {
  id: string;
  deleted: boolean;
  object: 'thread.deleted';
};
export type ModifyMessageRequest = {
  metadata?: JsonifiableObject | null;
};
export type DeleteMessageResponse = {
  id: string;
  deleted: boolean;
  object: 'thread.message.deleted';
};
/**
 * References an image [File](/docs/api-reference/files) in the content of a
 * message.
 */
export type MessageContentImageFileObject = {
  /**
   * Always `image_file`.
   * @enum image_file
   */
  type: 'image_file';
  image_file: {
    file_id: string;
  };
};
/** The text content that is part of a message. */
export type MessageContentTextObject = {
  /**
   * Always `text`.
   * @enum text
   */
  type: 'text';
  text: {
    value: string;
    annotations: Array<never | never | null>;
  };
};
/**
 * A citation within the message that points to a specific quote from a
 * specific File associated with the assistant or the message. Generated when
 * the assistant uses the "retrieval" tool to search files.
 */
export type MessageContentTextAnnotationsFileCitationObject = {
  /**
   * Always `file_citation`.
   * @enum file_citation
   */
  type: 'file_citation';
  text: string;
  file_citation: {
    file_id: string;
    quote: string;
  };
  start_index: number;
  end_index: number;
};
/**
 * A URL for the file that's generated when the assistant used the
 * `code_interpreter` tool to generate a file.
 */
export type MessageContentTextAnnotationsFilePathObject = {
  /**
   * Always `file_path`.
   * @enum file_path
   */
  type: 'file_path';
  text: string;
  file_path: {
    file_id: string;
  };
  start_index: number;
  end_index: number;
};
/** Details of the Code Interpreter tool call the run step was involved in. */
export type RunStepDetailsToolCallsCodeObject = {
  id: string;
  /**
   * The type of tool call. This is always going to be `code_interpreter` for
   * this type of tool call.
   * @enum code_interpreter
   */
  type: 'code_interpreter';
  code_interpreter: {
    input: string;
    outputs: Array<never | never | null>;
  };
};
/** Text output from the Code Interpreter tool call as part of a run step. */
export type RunStepDetailsToolCallsCodeOutputLogsObject = {
  /**
   * Always `logs`.
   * @enum logs
   */
  type: 'logs';
  logs: string;
};
export type RunStepDetailsToolCallsCodeOutputImageObject = {
  /**
   * Always `image`.
   * @enum image
   */
  type: 'image';
  image: {
    file_id: string;
  };
};
export type RunStepDetailsToolCallsRetrievalObject = {
  id: string;
  /**
   * The type of tool call. This is always going to be `retrieval` for this type
   * of tool call.
   * @enum retrieval
   */
  type: 'retrieval';
  retrieval: JsonifiableObject;
};
export type RunStepDetailsToolCallsFunctionObject = {
  id: string;
  /**
   * The type of tool call. This is always going to be `function` for this type
   * of tool call.
   * @enum function
   */
  type: 'function';
  function: {
    name: string;
    arguments: string;
    output: string | null;
  };
};
export type CreateAssistantFileRequest = {
  file_id: string;
};
/**
 * Deletes the association between the assistant and the file, but does not
 * delete the [File](/docs/api-reference/files) object itself.
 */
export type DeleteAssistantFileResponse = {
  id: string;
  deleted: boolean;
  object: 'assistant.file.deleted';
};
/** A list of files attached to a `message`. */
export type MessageFileObject = {
  id: string;
  /**
   * The object type, which is always `thread.message.file`.
   * @enum thread.message.file
   */
  object: 'thread.message.file';
  created_at: number;
  message_id: string;
};
export type ListMessageFilesResponse = {
  object: string;
  data: MessageFileObject[];
  first_id: string;
  last_id: string;
  has_more: boolean;
};
/** A list of [Files](/docs/api-reference/files) attached to an `assistant`. */
export type AssistantFileObject = {
  id: string;
  /**
   * The object type, which is always `assistant.file`.
   * @enum assistant.file
   */
  object: 'assistant.file';
  created_at: number;
  assistant_id: string;
};
export type ListAssistantFilesResponse = {
  object: string;
  data: AssistantFileObject[];
  first_id: string;
  last_id: string;
  has_more: boolean;
};
/** Details of the tool call. */
export type RunStepDetailsToolCallsObject = {
  /**
   * Always `tool_calls`.
   * @enum tool_calls
   */
  type: 'tool_calls';
  tool_calls: Array<
    | RunStepDetailsToolCallsCodeObject
    | RunStepDetailsToolCallsRetrievalObject
    | RunStepDetailsToolCallsFunctionObject
    | null
  >;
};
/** Details of the message creation by the run step. */
export type RunStepDetailsMessageCreationObject = {
  /**
   * Always `message_creation``.
   * @enum message_creation
   */
  type: 'message_creation';
  message_creation: {
    message_id: string;
  };
};
/** Represents a step in execution of a run. */
export type RunStepObject = {
  id: string;
  /**
   * The object type, which is always `thread.run.step``.
   * @enum thread.run.step
   */
  object: 'thread.run.step';
  created_at: number;
  assistant_id: string;
  thread_id: string;
  run_id: string;
  /**
   * The type of run step, which can be either `message_creation` or
   * `tool_calls`.
   * @enum message_creation,tool_calls
   */
  type: 'message_creation' | 'tool_calls';
  /**
   * The status of the run step, which can be either `in_progress`, `cancelled`,
   * `failed`, `completed`, or `expired`.
   * @enum in_progress,cancelled,failed,completed,expired
   */
  status: 'in_progress' | 'cancelled' | 'failed' | 'completed' | 'expired';
  step_details:
    | RunStepDetailsMessageCreationObject
    | RunStepDetailsToolCallsObject
    | null;
  last_error: {
    /**
     * One of `server_error` or `rate_limit_exceeded`.
     * @enum server_error,rate_limit_exceeded
     */
    code: 'server_error' | 'rate_limit_exceeded';
    message: string;
  };
  expired_at: number | null;
  cancelled_at: number | null;
  failed_at: number | null;
  completed_at: number | null;
  metadata: JsonifiableObject | null;
};
export type ListRunStepsResponse = {
  object: string;
  data: RunStepObject[];
  first_id: string;
  last_id: string;
  has_more: boolean;
};
/** Represents a message within a [thread](/docs/api-reference/threads). */
export type MessageObject = {
  id: string;
  /**
   * The object type, which is always `thread.message`.
   * @enum thread.message
   */
  object: 'thread.message';
  created_at: number;
  thread_id: string;
  /**
   * The entity that produced the message. One of `user` or `assistant`.
   * @enum user,assistant
   */
  role: 'user' | 'assistant';
  content: Array<
    MessageContentImageFileObject | MessageContentTextObject | null
  >;
  assistant_id: string | null;
  run_id: string | null;
  /**
   * A list of [file](/docs/api-reference/files) IDs that the assistant should
   * use. Useful for tools like retrieval and code_interpreter that can access
   * files. A maximum of 10 files can be attached to a message.
   * @default
   */
  file_ids: string[];
  metadata: JsonifiableObject | null;
};
export type ListMessagesResponse = {
  object: string;
  data: MessageObject[];
  first_id: string;
  last_id: string;
  has_more: boolean;
};
/** Represents a thread that contains [messages](/docs/api-reference/messages). */
export type ThreadObject = {
  id: string;
  /**
   * The object type, which is always `thread`.
   * @enum thread
   */
  object: 'thread';
  created_at: number;
  metadata: JsonifiableObject | null;
};
export type ListThreadsResponse = {
  object: string;
  data: ThreadObject[];
  first_id: string;
  last_id: string;
  has_more: boolean;
};
export type CreateMessageRequest = {
  /**
   * The role of the entity that is creating the message. Currently only `user`
   * is supported.
   * @enum user
   */
  role: 'user';
  content: string;
  /**
   * A list of [File](/docs/api-reference/files) IDs that the message should
   * use. There can be a maximum of 10 files attached to a message. Useful for
   * tools like `retrieval` and `code_interpreter` that can access and use
   * files.
   * @default
   */
  file_ids?: string[];
  metadata?: JsonifiableObject | null;
};
export type CreateThreadRequest = {
  messages?: CreateMessageRequest[];
  metadata?: JsonifiableObject | null;
};
export type CreateThreadAndRunRequest = {
  assistant_id: string;
  thread?: CreateThreadRequest | undefined;
  model?: string | null | undefined;
  instructions?: string | null | undefined;
  tools?: Array<AssistantToolsCode | AssistantToolsRetrieval | never | null>;
  metadata?: JsonifiableObject | null;
};
/** Tool call objects */
export type RunToolCallObject = {
  id: string;
  /**
   * The type of tool call the output is required for. For now, this is always
   * `function`.
   * @enum function
   */
  type: 'function';
  function: {
    name: string;
    arguments: string;
  };
};
/** Represents an execution run on a [thread](/docs/api-reference/threads). */
export type RunObject = {
  id: string;
  /**
   * The object type, which is always `thread.run`.
   * @enum thread.run
   */
  object: 'thread.run';
  created_at: number;
  thread_id: string;
  assistant_id: string;
  /**
   * The status of the run, which can be either `queued`, `in_progress`,
   * `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`, or
   * `expired`.
   * @enum queued,in_progress,requires_action,cancelling,cancelled,failed,completed,expired
   */
  status:
    | 'queued'
    | 'in_progress'
    | 'requires_action'
    | 'cancelling'
    | 'cancelled'
    | 'failed'
    | 'completed'
    | 'expired';
  required_action: {
    /**
     * For now, this is always `submit_tool_outputs`.
     * @enum submit_tool_outputs
     */
    type: 'submit_tool_outputs';
    submit_tool_outputs: {
      tool_calls: RunToolCallObject[];
    };
  };
  last_error: {
    /**
     * One of `server_error` or `rate_limit_exceeded`.
     * @enum server_error,rate_limit_exceeded
     */
    code: 'server_error' | 'rate_limit_exceeded';
    message: string;
  };
  expires_at: number;
  started_at: number | null;
  cancelled_at: number | null;
  failed_at: number | null;
  completed_at: number | null;
  model: string;
  instructions: string;
  /**
   * The list of tools that the [assistant](/docs/api-reference/assistants) used
   * for this run.
   * @default
   */
  tools: Array<AssistantToolsCode | AssistantToolsRetrieval | never | null>;
  /**
   * The list of [File](/docs/api-reference/files) IDs the
   * [assistant](/docs/api-reference/assistants) used for this run.
   * @default
   */
  file_ids: string[];
  metadata: JsonifiableObject | null;
};
export type ListRunsResponse = {
  object: string;
  data: RunObject[];
  first_id: string;
  last_id: string;
  has_more: boolean;
};
/**
 * The parameters the functions accepts, described as a JSON Schema object.
 * See the [guide](/docs/guides/text-generation/function-calling) for
 * examples, and the [JSON Schema
 * reference](https://json-schema.org/understanding-json-schema/) for
 * documentation about the format.
 *
 * To describe a function that accepts no parameters, provide the value
 * `{"type": "object", "properties": {}}`.
 */
export type FunctionParameters = {};
export type FunctionObject = {
  description?: string | undefined;
  name: string;
  parameters: FunctionParameters;
};
export type AssistantToolsFunction = {
  /**
   * The type of tool being defined: `function`
   * @enum function
   */
  type: 'function';
  function: FunctionObject;
};
/** Represents an `assistant` that can call the model and use tools. */
export type AssistantObject = {
  id: string;
  /**
   * The object type, which is always `assistant`.
   * @enum assistant
   */
  object: 'assistant';
  created_at: number;
  name: string | null;
  description: string | null;
  model: string;
  instructions: string | null;
  /**
   * A list of tool enabled on the assistant. There can be a maximum of 128
   * tools per assistant. Tools can be of types `code_interpreter`, `retrieval`,
   * or `function`.
   * @default
   */
  tools: Array<
    AssistantToolsCode | AssistantToolsRetrieval | AssistantToolsFunction | null
  >;
  /**
   * A list of [file](/docs/api-reference/files) IDs attached to this assistant.
   * There can be a maximum of 20 files attached to the assistant. Files are
   * ordered by their creation date in ascending order.
   * @default
   */
  file_ids: string[];
  metadata: JsonifiableObject | null;
};
export type ListAssistantsResponse = {
  object: string;
  data: AssistantObject[];
  first_id: string;
  last_id: string;
  has_more: boolean;
};
/** Represents an embedding vector returned by embedding endpoint. */
export type Embedding = {
  index: number;
  embedding: number[];
  /**
   * The object type, which is always "embedding".
   * @enum embedding
   */
  object: 'embedding';
};
export type CreateEmbeddingResponse = {
  data: Embedding[];
  model: string;
  /**
   * The object type, which is always "list".
   * @enum list
   */
  object: 'list';
  usage: {
    prompt_tokens: number;
    total_tokens: number;
  };
};
/** Fine-tune event object */
export type FineTuneEvent = {
  created_at: number;
  level: string;
  message: string;
  object: 'fine-tune-event';
};
export type ListFineTuneEventsResponse = {
  data: FineTuneEvent[];
  object: 'list';
};
/** The `File` object represents a document that has been uploaded to OpenAI. */
export type OpenAiFile = {
  id: string;
  bytes: number;
  created_at: number;
  filename: string;
  /**
   * The object type, which is always `file`.
   * @enum file
   */
  object: 'file';
  /**
   * The intended purpose of the file. Supported values are `fine-tune`,
   * `fine-tune-results`, `assistants`, and `assistants_output`.
   * @enum fine-tune,fine-tune-results,assistants,assistants_output
   */
  purpose:
    | 'fine-tune'
    | 'fine-tune-results'
    | 'assistants'
    | 'assistants_output';
  /**
   * Deprecated. The current status of the file, which can be either `uploaded`,
   * `processed`, or `error`.
   * @enum uploaded,processed,error
   * @deprecated
   */
  status: 'uploaded' | 'processed' | 'error';
  /**
   * Deprecated. For details on why a fine-tuning training file failed
   * validation, see the `error` field on `fine_tuning.job`.
   * @deprecated
   */
  status_details: string;
};
/**
 * The `FineTune` object represents a legacy fine-tune job that has been
 * created through the API.
 */
export type FineTune = {
  id: string;
  created_at: number;
  events?: FineTuneEvent[];
  fine_tuned_model: string | null;
  hyperparams: {
    batch_size: number;
    classification_n_classes?: number;
    classification_positive_class?: string | undefined;
    compute_classification_metrics?: boolean | undefined;
    learning_rate_multiplier: number;
    n_epochs: number;
    prompt_loss_weight: number;
  };
  model: string;
  /**
   * The object type, which is always "fine-tune".
   * @enum fine-tune
   */
  object: 'fine-tune';
  organization_id: string;
  result_files: OpenAiFile[];
  status: string;
  training_files: OpenAiFile[];
  updated_at: number;
  validation_files: OpenAiFile[];
};
export type ListFineTunesResponse = {
  data: FineTune[];
  object: 'list';
};
/** Fine-tuning job event object */
export type FineTuningJobEvent = {
  id: string;
  created_at: number;
  level: 'info' | 'warn' | 'error';
  message: string;
  object: 'fine_tuning.job.event';
};
export type ListFineTuningJobEventsResponse = {
  data: FineTuningJobEvent[];
  object: 'list';
};
export type ListFilesResponse = {
  data: OpenAiFile[];
  object: 'list';
};
/** Represents the url or the content of an image generated by the OpenAI API. */
export type Image = {
  b64_json?: string | undefined;
  url?: string | undefined;
  revised_prompt?: string | undefined;
};
export type ImagesResponse = {
  created: number;
  data: Image[];
};
/** Usage statistics for the completion request. */
export type CompletionUsage = {
  completion_tokens: number;
  prompt_tokens: number;
  total_tokens: number;
};
export type CreateEditResponse = {
  choices: Array<{
    /**
     * The reason the model stopped generating tokens. This will be `stop` if the
     * model hit a natural stop point or a provided stop sequence,
     * `length` if the maximum number of tokens specified in the request was
     * reached,
     * or `content_filter` if content was omitted due to a flag from our content
     * filters.
     * @enum stop,length
     */
    finish_reason: 'stop' | 'length';
    index: number;
    text: string;
  }>;
  /**
   * The object type, which is always `edit`.
   * @enum edit
   */
  object: 'edit';
  created: number;
  usage: CompletionUsage;
};
/**
 * The `fine_tuning.job` object represents a fine-tuning job that has been
 * created through the API.
 */
export type FineTuningJob = {
  id: string;
  created_at: number;
  error: {
    code: string;
    message: string;
    param: string | null;
  };
  fine_tuned_model: string | null;
  finished_at: number | null;
  hyperparameters: {
    /**
     * The number of epochs to train the model for. An epoch refers to one full
     * cycle through the training dataset.
     * "auto" decides the optimal number of epochs based on the size of the
     * dataset. If setting the number manually, we support any number between 1
     * and 50 epochs.
     * @default auto
     */
    n_epochs: 'auto' | number | null;
  };
  model: string;
  /**
   * The object type, which is always "fine_tuning.job".
   * @enum fine_tuning.job
   */
  object: 'fine_tuning.job';
  organization_id: string;
  result_files: string[];
  /**
   * The current status of the fine-tuning job, which can be either
   * `validating_files`, `queued`, `running`, `succeeded`, `failed`, or
   * `cancelled`.
   * @enum validating_files,queued,running,succeeded,failed,cancelled
   */
  status:
    | 'validating_files'
    | 'queued'
    | 'running'
    | 'succeeded'
    | 'failed'
    | 'cancelled';
  trained_tokens: number | null;
  training_file: string;
  validation_file: string | null;
};
export type ListPaginatedFineTuningJobsResponse = {
  data: FineTuningJob[];
  has_more: boolean;
  object: 'list';
};
/**
 * Represents a chat completion response returned by model, based on the
 * provided input.
 */
export type CreateChatCompletionFunctionResponse = {
  id: string;
  choices: Array<{
    /**
     * The reason the model stopped generating tokens. This will be `stop` if the
     * model hit a natural stop point or a provided stop sequence, `length` if the
     * maximum number of tokens specified in the request was reached,
     * `content_filter` if content was omitted due to a flag from our content
     * filters, or `function_call` if the model called a function.
     * @enum stop,length,function_call,content_filter
     */
    finish_reason: 'stop' | 'length' | 'function_call' | 'content_filter';
    index: number;
    /** WARN: $ref used before available - #/components/schemas/ChatCompletionResponseMessage */
    message: never;
  }>;
  created: number;
  model: string;
  system_fingerprint?: string | undefined;
  /**
   * The object type, which is always `chat.completion`.
   * @enum chat.completion
   */
  object: 'chat.completion';
  usage?: CompletionUsage | undefined;
};
/**
 * Represents a chat completion response returned by model, based on the
 * provided input.
 */
export type CreateChatCompletionResponse = {
  id: string;
  choices: Array<{
    /**
     * The reason the model stopped generating tokens. This will be `stop` if the
     * model hit a natural stop point or a provided stop sequence,
     * `length` if the maximum number of tokens specified in the request was
     * reached,
     * `content_filter` if content was omitted due to a flag from our content
     * filters,
     * `tool_calls` if the model called a tool, or `function_call` (deprecated) if
     * the model called a function.
     * @enum stop,length,tool_calls,content_filter,function_call
     */
    finish_reason:
      | 'stop'
      | 'length'
      | 'tool_calls'
      | 'content_filter'
      | 'function_call';
    index: number;
    /** WARN: $ref used before available - #/components/schemas/ChatCompletionResponseMessage */
    message: never;
  }>;
  created: number;
  model: string;
  system_fingerprint?: string | undefined;
  /**
   * The object type, which is always `chat.completion`.
   * @enum chat.completion
   */
  object: 'chat.completion';
  usage?: CompletionUsage | undefined;
};
/**
 * Specifying a particular function via `{"name": "my_function"}` forces the
 * model to call that function.
 */
export type ChatCompletionFunctionCallOption = {
  name: string;
};
export type ChatCompletionFunctions = {
  description?: string | undefined;
  name: string;
  parameters: FunctionParameters;
};
/**
 * Specifies a tool the model should use. Use to force the model to call a
 * specific function.
 */
export type ChatCompletionNamedToolChoice = {
  /**
   * The type of the tool. Currently, only `function` is supported.
   * @enum function
   */
  type: 'function';
  function: {
    name: string;
  };
};
/**
 * Controls which (if any) function is called by the model.
 * `none` means the model will not call a function and instead generates a
 * message.
 * `auto` means the model can pick between generating a message or calling a
 * function.
 * Specifying a particular function via `{"type: "function", "function":
 * {"name": "my_function"}}` forces the model to call that function.
 *
 * `none` is the default when no functions are present. `auto` is the default
 * if functions are present.
 */
export type ChatCompletionToolChoiceOption =
  | ChatCompletionNamedToolChoice
  | 'none'
  | 'auto';
export type ChatCompletionTool = {
  /**
   * The type of the tool. Currently, only `function` is supported.
   * @enum function
   */
  type: 'function';
  function: FunctionObject;
};
export type ChatCompletionRequestFunctionMessage = {
  /**
   * The role of the messages author, in this case `function`.
   * @enum function
   */
  role: 'function';
  content: string;
  name: string;
};
export type ChatCompletionRequestToolMessage = {
  /**
   * The role of the messages author, in this case `tool`.
   * @enum tool
   */
  role: 'tool';
  content: string;
  tool_call_id: string;
};
export type ChatCompletionMessageToolCall = {
  id: string;
  /**
   * The type of the tool. Currently, only `function` is supported.
   * @enum function
   */
  type: 'function';
  function: {
    name: string;
    arguments: string;
  };
};
/** The tool calls generated by the model, such as function calls. */
export type ChatCompletionMessageToolCalls = ChatCompletionMessageToolCall[];
export type ChatCompletionRequestAssistantMessage = {
  content?: string | null | undefined;
  /**
   * The role of the messages author, in this case `assistant`.
   * @enum assistant
   */
  role: 'assistant';
  name?: string | undefined;
  tool_calls?: ChatCompletionMessageToolCalls | undefined;
  /**
   * Deprecated and replaced by `tool_calls`. The name and arguments of a
   * function that should be called, as generated by the model.
   * @deprecated
   */
  function_call?:
    | {
        arguments: string;
        name: string;
      }
    | undefined;
};
export type ChatCompletionRequestMessageContentPartImage = {
  /**
   * The type of the content part.
   * @enum image_url
   */
  type: 'image_url';
  image_url: {
    url: string;
    /**
     * Specifies the detail level of the image. Learn more in the [Vision
     * guide](/docs/guides/vision/low-or-high-fidelity-image-understanding).
     * @default auto
     * @enum auto,low,high
     */
    detail?: 'auto' | 'low' | 'high' | undefined;
  };
};
export type ChatCompletionRequestMessageContentPartText = {
  /**
   * The type of the content part.
   * @enum text
   */
  type: 'text';
  text: string;
};
export type ChatCompletionRequestMessageContentPart =
  | ChatCompletionRequestMessageContentPartText
  | ChatCompletionRequestMessageContentPartImage;
export type ChatCompletionRequestUserMessage = {
  content: string | ChatCompletionRequestMessageContentPart[] | null;
  /**
   * The role of the messages author, in this case `user`.
   * @enum user
   */
  role: 'user';
  name?: string | undefined;
};
export type ChatCompletionRequestSystemMessage = {
  content: string;
  /**
   * The role of the messages author, in this case `system`.
   * @enum system
   */
  role: 'system';
  name?: string | undefined;
};
export type ChatCompletionRequestMessage =
  | ChatCompletionRequestSystemMessage
  | ChatCompletionRequestUserMessage
  | ChatCompletionRequestAssistantMessage
  | ChatCompletionRequestToolMessage
  | ChatCompletionRequestFunctionMessage;
export type CreateChatCompletionRequest = {
  messages: ChatCompletionRequestMessage[];
  /**
   * ID of the model to use. See the [model endpoint
   * compatibility](/docs/models/model-endpoint-compatibility) table for details
   * on which models work with the Chat API.
   * @example gpt-3.5-turbo
   */
  model:
    | string
    | 'gpt-4-1106-preview'
    | 'gpt-4-vision-preview'
    | 'gpt-4'
    | 'gpt-4-0314'
    | 'gpt-4-0613'
    | 'gpt-4-32k'
    | 'gpt-4-32k-0314'
    | 'gpt-4-32k-0613'
    | 'gpt-3.5-turbo'
    | 'gpt-3.5-turbo-16k'
    | 'gpt-3.5-turbo-0301'
    | 'gpt-3.5-turbo-0613'
    | 'gpt-3.5-turbo-1106'
    | 'gpt-3.5-turbo-16k-0613'
    | null;
  frequency_penalty?: number | null;
  logit_bias?: JsonifiableObject | null;
  max_tokens?: number | null;
  /**
   * How many chat completion choices to generate for each input message. Note
   * that you will be charged based on the number of generated tokens across all
   * of the choices. Keep `n` as `1` to minimize costs.
   * @default 1
   * @example 1
   */
  n?: number | null;
  presence_penalty?: number | null;
  response_format?:
    | {
        /**
         * Must be one of `text` or `json_object`.
         * @default text
         * @enum text,json_object
         * @example json_object
         */
        type?: 'text' | 'json_object' | undefined;
      }
    | undefined;
  seed?: number | null;
  stop?: string | null | string[] | null | undefined;
  stream?: boolean | null | undefined;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8
   * will make the output more random, while lower values like 0.2 will make it
   * more focused and deterministic.
   *
   * We generally recommend altering this or `top_p` but not both.
   * @default 1
   * @example 1
   */
  temperature?: number | null;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where
   * the model considers the results of the tokens with top_p probability mass.
   * So 0.1 means only the tokens comprising the top 10% probability mass are
   * considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   * @default 1
   * @example 1
   */
  top_p?: number | null;
  tools?: ChatCompletionTool[];
  tool_choice?: ChatCompletionToolChoiceOption | undefined;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to
   * monitor and detect abuse. [Learn
   * more](/docs/guides/safety-best-practices/end-user-ids).
   * @example user-1234
   */
  user?: string | undefined;
  /**
   * Deprecated in favor of `tool_choice`.
   *
   * Controls which (if any) function is called by the model.
   * `none` means the model will not call a function and instead generates a
   * message.
   * `auto` means the model can pick between generating a message or calling a
   * function.
   * Specifying a particular function via `{"name": "my_function"}` forces the
   * model to call that function.
   *
   * `none` is the default when no functions are present. `auto`` is the default
   * if functions are present.
   * @deprecated
   */
  function_call?:
    | 'none'
    | 'auto'
    | ChatCompletionFunctionCallOption
    | null
    | undefined;
  /**
   * Deprecated in favor of `tools`.
   *
   * A list of functions the model may generate JSON inputs for.
   * @deprecated
   */
  functions?: ChatCompletionFunctions[];
};
export type ChatCompletionMessageToolCallChunk = {
  index: number;
  id?: string | undefined;
  /**
   * The type of the tool. Currently, only `function` is supported.
   * @enum function
   */
  type?: 'function' | undefined;
  function?:
    | {
        name?: string | undefined;
        arguments?: string | undefined;
      }
    | undefined;
};
/** A chat completion delta generated by streamed model responses. */
export type ChatCompletionStreamResponseDelta = {
  content?: string | null | undefined;
  /**
   * Deprecated and replaced by `tool_calls`. The name and arguments of a
   * function that should be called, as generated by the model.
   * @deprecated
   */
  function_call?:
    | {
        arguments?: string | undefined;
        name?: string | undefined;
      }
    | undefined;
  tool_calls?: ChatCompletionMessageToolCallChunk[];
  /**
   * The role of the author of this message.
   * @enum system,user,assistant,tool
   */
  role?: 'system' | 'user' | 'assistant' | 'tool' | undefined;
};
/** A chat completion message generated by the model. */
export type ChatCompletionResponseMessage = {
  content: string | null;
  tool_calls?: ChatCompletionMessageToolCalls | undefined;
  /**
   * The role of the author of this message.
   * @enum assistant
   */
  role: 'assistant';
  /**
   * Deprecated and replaced by `tool_calls`. The name and arguments of a
   * function that should be called, as generated by the model.
   * @deprecated
   */
  function_call?:
    | {
        arguments: string;
        name: string;
      }
    | undefined;
};
/**
 * Represents a completion response from the API. Note: both the streamed and
 * non-streamed response objects share the same shape (unlike the chat
 * endpoint).
 */
export type CreateCompletionResponse = {
  id: string;
  choices: Array<{
    /**
     * The reason the model stopped generating tokens. This will be `stop` if the
     * model hit a natural stop point or a provided stop sequence,
     * `length` if the maximum number of tokens specified in the request was
     * reached,
     * or `content_filter` if content was omitted due to a flag from our content
     * filters.
     * @enum stop,length,content_filter
     */
    finish_reason: 'stop' | 'length' | 'content_filter';
    index: number;
    logprobs: {
      text_offset?: number[];
      token_logprobs?: number[];
      tokens?: string[];
      top_logprobs?: JsonifiableObject[];
    };
    text: string;
  }>;
  created: number;
  model: string;
  system_fingerprint?: string | undefined;
  /**
   * The object type, which is always "text_completion"
   * @enum text_completion
   */
  object: 'text_completion';
  usage?: CompletionUsage | undefined;
};
/** Describes an OpenAI model offering that can be used with the API. */
export type Model = {
  id: string;
  created: number;
  /**
   * The object type, which is always "model".
   * @enum model
   */
  object: 'model';
  owned_by: string;
};
export type ListModelsResponse = {
  object: 'list';
  data: Model[];
};
export type Error = {
  code: string | null;
  message: string;
  param: string | null;
  type: string;
};
export type ErrorResponse = {
  error: Error;
};
export type CreateChatCompletionCommandInput = CreateChatCompletionRequest;
export type CreateChatCompletionCommandBody = CreateChatCompletionRequest;
export type CreateCompletionCommandInput = CreateCompletionRequest;
export type CreateCompletionCommandBody = CreateCompletionRequest;
export type CreateEditCommandInput = CreateEditRequest;
export type CreateEditCommandBody = CreateEditRequest;
export type CreateImageCommandInput = CreateImageRequest;
export type CreateImageCommandBody = CreateImageRequest;
export type CreateImageEditCommandInput = void;
export type CreateImageEditCommandBody = void;
export type CreateImageVariationCommandInput = void;
export type CreateImageVariationCommandBody = void;
export type CreateEmbeddingCommandInput = CreateEmbeddingRequest;
export type CreateEmbeddingCommandBody = CreateEmbeddingRequest;
export type CreateSpeechCommandInput = CreateSpeechRequest;
export type CreateSpeechCommandBody = CreateSpeechRequest;
export type CreateTranscriptionCommandInput = void;
export type CreateTranscriptionCommandBody = void;
export type CreateTranslationCommandInput = void;
export type CreateTranslationCommandBody = void;
export type ListFilesCommandQuery = {
  purpose: string;
};
export type ListFilesCommandInput = ListFilesCommandQuery;
export type ListFilesCommandBody = void;
export type CreateFileCommandInput = void;
export type CreateFileCommandBody = void;
export type DeleteFileCommandParams = {
  fileId: string;
};
export type DeleteFileCommandInput = DeleteFileCommandParams;
export type DeleteFileCommandBody = void;
export type RetrieveFileCommandParams = {
  fileId: string;
};
export type RetrieveFileCommandInput = RetrieveFileCommandParams;
export type RetrieveFileCommandBody = void;
export type DownloadFileCommandParams = {
  fileId: string;
};
export type DownloadFileCommandInput = DownloadFileCommandParams;
export type DownloadFileCommandBody = void;
export type CreateFineTuningJobCommandInput = CreateFineTuningJobRequest;
export type CreateFineTuningJobCommandBody = CreateFineTuningJobRequest;
export type ListPaginatedFineTuningJobsCommandQuery = {
  after: string;
  limit: `${number}`;
};
export type ListPaginatedFineTuningJobsCommandInput =
  ListPaginatedFineTuningJobsCommandQuery;
export type ListPaginatedFineTuningJobsCommandBody = void;
export type RetrieveFineTuningJobCommandParams = {
  fineTuningJobId: string;
};
export type RetrieveFineTuningJobCommandInput =
  RetrieveFineTuningJobCommandParams;
export type RetrieveFineTuningJobCommandBody = void;
export type ListFineTuningEventsCommandQuery = {
  after: string;
  limit: `${number}`;
};
export type ListFineTuningEventsCommandParams = {
  fineTuningJobId: string;
};
export type ListFineTuningEventsCommandInput =
  ListFineTuningEventsCommandQuery & ListFineTuningEventsCommandParams;
export type ListFineTuningEventsCommandBody = void;
export type CancelFineTuningJobCommandParams = {
  fineTuningJobId: string;
};
export type CancelFineTuningJobCommandInput = CancelFineTuningJobCommandParams;
export type CancelFineTuningJobCommandBody = void;
export type CreateFineTuneCommandInput = CreateFineTuneRequest;
export type CreateFineTuneCommandBody = CreateFineTuneRequest;
export type ListFineTunesCommandInput = void;
export type ListFineTunesCommandBody = void;
export type RetrieveFineTuneCommandParams = {
  fineTuneId: string;
};
export type RetrieveFineTuneCommandInput = RetrieveFineTuneCommandParams;
export type RetrieveFineTuneCommandBody = void;
export type CancelFineTuneCommandParams = {
  fineTuneId: string;
};
export type CancelFineTuneCommandInput = CancelFineTuneCommandParams;
export type CancelFineTuneCommandBody = void;
export type ListFineTuneEventsCommandQuery = {
  stream: 'true' | 'false';
};
export type ListFineTuneEventsCommandParams = {
  fineTuneId: string;
};
export type ListFineTuneEventsCommandInput = ListFineTuneEventsCommandQuery &
  ListFineTuneEventsCommandParams;
export type ListFineTuneEventsCommandBody = void;
export type ListModelsCommandInput = void;
export type ListModelsCommandBody = void;
export type RetrieveModelCommandParams = {
  model: string;
};
export type RetrieveModelCommandInput = RetrieveModelCommandParams;
export type RetrieveModelCommandBody = void;
export type DeleteModelCommandParams = {
  model: string;
};
export type DeleteModelCommandInput = DeleteModelCommandParams;
export type DeleteModelCommandBody = void;
export type CreateModerationCommandInput = CreateModerationRequest;
export type CreateModerationCommandBody = CreateModerationRequest;
export type ListAssistantsCommandQuery = {
  limit: `${number}`;
  order: 'asc' | 'desc';
  after: string;
  before: string;
};
export type ListAssistantsCommandInput = ListAssistantsCommandQuery;
export type ListAssistantsCommandBody = void;
export type CreateAssistantCommandInput = CreateAssistantRequest;
export type CreateAssistantCommandBody = CreateAssistantRequest;
export type GetAssistantCommandParams = {
  assistantId: string;
};
export type GetAssistantCommandInput = GetAssistantCommandParams;
export type GetAssistantCommandBody = void;
export type ModifyAssistantCommandParams = {
  assistantId: string;
};
export type ModifyAssistantCommandInput = ModifyAssistantRequest &
  ModifyAssistantCommandParams;
export type ModifyAssistantCommandBody = ModifyAssistantRequest;
export type DeleteAssistantCommandParams = {
  assistantId: string;
};
export type DeleteAssistantCommandInput = DeleteAssistantCommandParams;
export type DeleteAssistantCommandBody = void;
export type CreateThreadCommandInput = CreateThreadRequest;
export type CreateThreadCommandBody = CreateThreadRequest;
export type GetThreadCommandParams = {
  threadId: string;
};
export type GetThreadCommandInput = GetThreadCommandParams;
export type GetThreadCommandBody = void;
export type ModifyThreadCommandParams = {
  threadId: string;
};
export type ModifyThreadCommandInput = ModifyThreadRequest &
  ModifyThreadCommandParams;
export type ModifyThreadCommandBody = ModifyThreadRequest;
export type DeleteThreadCommandParams = {
  threadId: string;
};
export type DeleteThreadCommandInput = DeleteThreadCommandParams;
export type DeleteThreadCommandBody = void;
export type ListMessagesCommandQuery = {
  limit: `${number}`;
  order: 'asc' | 'desc';
  after: string;
  before: string;
};
export type ListMessagesCommandParams = {
  threadId: string;
};
export type ListMessagesCommandInput = ListMessagesCommandQuery &
  ListMessagesCommandParams;
export type ListMessagesCommandBody = void;
export type CreateMessageCommandParams = {
  threadId: string;
};
export type CreateMessageCommandInput = CreateMessageRequest &
  CreateMessageCommandParams;
export type CreateMessageCommandBody = CreateMessageRequest;
export type GetMessageCommandParams = {
  threadId: string;
  messageId: string;
};
export type GetMessageCommandInput = GetMessageCommandParams;
export type GetMessageCommandBody = void;
export type ModifyMessageCommandParams = {
  threadId: string;
  messageId: string;
};
export type ModifyMessageCommandInput = ModifyMessageRequest &
  ModifyMessageCommandParams;
export type ModifyMessageCommandBody = ModifyMessageRequest;
export type CreateThreadAndRunCommandInput = CreateThreadAndRunRequest;
export type CreateThreadAndRunCommandBody = CreateThreadAndRunRequest;
export type ListRunsCommandQuery = {
  limit: `${number}`;
  order: 'asc' | 'desc';
  after: string;
  before: string;
};
export type ListRunsCommandParams = {
  threadId: string;
};
export type ListRunsCommandInput = ListRunsCommandQuery & ListRunsCommandParams;
export type ListRunsCommandBody = void;
export type CreateRunCommandParams = {
  threadId: string;
};
export type CreateRunCommandInput = CreateRunRequest & CreateRunCommandParams;
export type CreateRunCommandBody = CreateRunRequest;
export type GetRunCommandParams = {
  threadId: string;
  runId: string;
};
export type GetRunCommandInput = GetRunCommandParams;
export type GetRunCommandBody = void;
export type ModifyRunCommandParams = {
  threadId: string;
  runId: string;
};
export type ModifyRunCommandInput = ModifyRunRequest & ModifyRunCommandParams;
export type ModifyRunCommandBody = ModifyRunRequest;
export type SubmitToolOuputsToRunCommandParams = {
  threadId: string;
  runId: string;
};
export type SubmitToolOuputsToRunCommandInput = SubmitToolOutputsRunRequest &
  SubmitToolOuputsToRunCommandParams;
export type SubmitToolOuputsToRunCommandBody = SubmitToolOutputsRunRequest;
export type CancelRunCommandParams = {
  threadId: string;
  runId: string;
};
export type CancelRunCommandInput = CancelRunCommandParams;
export type CancelRunCommandBody = void;
export type ListRunStepsCommandQuery = {
  limit: `${number}`;
  order: 'asc' | 'desc';
  after: string;
  before: string;
};
export type ListRunStepsCommandParams = {
  threadId: string;
  runId: string;
};
export type ListRunStepsCommandInput = ListRunStepsCommandQuery &
  ListRunStepsCommandParams;
export type ListRunStepsCommandBody = void;
export type GetRunStepCommandParams = {
  threadId: string;
  runId: string;
  stepId: string;
};
export type GetRunStepCommandInput = GetRunStepCommandParams;
export type GetRunStepCommandBody = void;
export type ListAssistantFilesCommandQuery = {
  limit: `${number}`;
  order: 'asc' | 'desc';
  after: string;
  before: string;
};
export type ListAssistantFilesCommandParams = {
  assistantId: string;
};
export type ListAssistantFilesCommandInput = ListAssistantFilesCommandQuery &
  ListAssistantFilesCommandParams;
export type ListAssistantFilesCommandBody = void;
export type CreateAssistantFileCommandParams = {
  assistantId: string;
};
export type CreateAssistantFileCommandInput = CreateAssistantFileRequest &
  CreateAssistantFileCommandParams;
export type CreateAssistantFileCommandBody = CreateAssistantFileRequest;
export type GetAssistantFileCommandParams = {
  assistantId: string;
  fileId: string;
};
export type GetAssistantFileCommandInput = GetAssistantFileCommandParams;
export type GetAssistantFileCommandBody = void;
export type DeleteAssistantFileCommandParams = {
  assistantId: string;
  fileId: string;
};
export type DeleteAssistantFileCommandInput = DeleteAssistantFileCommandParams;
export type DeleteAssistantFileCommandBody = void;
export type ListMessageFilesCommandQuery = {
  limit: `${number}`;
  order: 'asc' | 'desc';
  after: string;
  before: string;
};
export type ListMessageFilesCommandParams = {
  threadId: string;
  messageId: string;
};
export type ListMessageFilesCommandInput = ListMessageFilesCommandQuery &
  ListMessageFilesCommandParams;
export type ListMessageFilesCommandBody = void;
export type GetMessageFileCommandParams = {
  threadId: string;
  messageId: string;
  fileId: string;
};
export type GetMessageFileCommandInput = GetMessageFileCommandParams;
export type GetMessageFileCommandBody = void;
